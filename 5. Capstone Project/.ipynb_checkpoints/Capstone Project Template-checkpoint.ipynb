{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipleine for Immigration Data\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project is to create a database that is optimised to query immigration events. This will be used by Data Analysts to determine if temperature affects the selection of destination cities.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the the libraries\n",
    "import pandas as pd, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "##### Goal:\n",
    "\n",
    "The goal of this project is to create a database that is optimised to query immigration events. This will be used by Data Analysts to determine if temperature affects the selection of destination cities.\n",
    "\n",
    "##### Schema:\n",
    "\n",
    "Schema consists of 1 Fact table and 2 Dimension tables. \n",
    "\n",
    "In this project, we will aggregate I94 immigration data by destination city to form our first dimension table. Next we will aggregate city temperature data by city to form the second dimension table. The two datasets will be joined on destination city to form the fact table. The final database is optimized to query on immigration events to determine  Spark will be used to process the data.\n",
    "\n",
    "#### Data Description \n",
    "\n",
    "The I94 immigration data comes from the US National Tourism and Trade Office. It is provided in SAS7BDAT format which is a binary database storage format. Some relevant attributes include:\n",
    "\n",
    "- i94yr = 4 digit year\n",
    "- i94mon = numeric month\n",
    "- i94cit = 3 digit code of origin city\n",
    "- i94port = 3 character code of destination USA city\n",
    "- arrdate = arrival date in the USA\n",
    "- i94mode = 1 digit travel code\n",
    "- depdate = departure date from the USA\n",
    "- i94visa = reason for immigration\n",
    "\n",
    "The temperature data comes from Kaggle. It is provided in csv format. Some relevant attributes include:\n",
    "\n",
    "- AverageTemperature = average temperature\n",
    "- City = city name\n",
    "- Country = country name\n",
    "- Latitude= latitude\n",
    "- Longitude = longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read April 2016 I94 immigration data into Pandas for exploration\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few entries of temperature data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with SAS7BDAT jar\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Cleaning\n",
    "\n",
    "For the I94 immigration data, we want to drop all entries where the destination city code `i94port` is not a valid value (e.g., `XXX`, `99`, etc) as described in `I94_SAS_Labels_Description.SAS`. For the temperature data, we want to drop all entries where `AverageTemperature` is NaN, then drop all entries with duplicate locations, and then add the `i94port` of the location in each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean I94 immigration data\n",
    "\n",
    "# Create dictionary of valid i94port codes\n",
    "re_obj = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "i94port_valid = {}\n",
    "with open('i94port_valid.txt') as f:\n",
    "     for line in f:\n",
    "         match = re_obj.search(line)\n",
    "         i94port_valid[match[1]]=[match[2]]\n",
    "\n",
    "def clean_i94_data(file):\n",
    "    '''\n",
    "    Input: Path to I94 immigration data file\n",
    "    \n",
    "    Output: Spark dataframe of I94 immigration data with valid i94port\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Read I94 data into Spark\n",
    "    df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "\n",
    "    # Filter out entries where i94port is invalid\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(i94port_valid.keys())))\n",
    "\n",
    "    return df_immigration\n",
    "\n",
    "# Test function\n",
    "# immigration_test_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat' \n",
    "# df_immigration_test = clean_i94_data(immigration_test_file)\n",
    "# df_immigration_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "| 18.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MI|20555.0|  57.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247103803E10|00602|      B1|\n",
      "| 19.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  63.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1953.0|09302016|  null|  null|     AZ|9.247139923E10|00602|      B2|\n",
      "| 20.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  57.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247161383E10|00602|      B2|\n",
      "| 21.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20553.0|  46.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1970.0|09302016|  null|  null|     AZ|9.247079603E10|00602|      B2|\n",
      "| 22.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20562.0|  48.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1968.0|09302016|  null|  null|     AZ|9.247848973E10|00608|      B1|\n",
      "| 23.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20671.0|  52.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1964.0|09302016|  null|  null|     TK|9.250139443E10|00001|      B2|\n",
      "| 24.0|2016.0|   4.0| 101.0| 101.0|    TOR|20545.0|    1.0|     MO|20554.0|  33.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1983.0|09302016|  null|  null|     MQ|9.249090503E10|03348|      B2|\n",
      "| 27.0|2016.0|   4.0| 101.0| 101.0|    BOS|20545.0|    1.0|     MA|20549.0|  58.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1958.0|04062016|     M|  null|     LH|9.247876383E10|00422|      B1|\n",
      "| 28.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20549.0|  56.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1960.0|04062016|     F|  null|     LH|9.247890033E10|00422|      B1|\n",
      "| 29.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20561.0|  62.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1954.0|09302016|     M|  null|     AZ|9.250378143E10|00614|      B2|\n",
      "| 30.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NJ|20578.0|  49.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1967.0|09302016|     M|  null|     OS|9.247020943E10|00089|      B2|\n",
      "| 31.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NY|20611.0|  43.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1973.0|09302016|     M|  null|     OS|9.247128923E10|00089|      B2|\n",
      "| 33.0|2016.0|   4.0| 101.0| 101.0|    HOU|20545.0|    1.0|     TX|20554.0|  53.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1963.0|09302016|     F|  null|     TK|9.250930163E10|00033|      B2|\n",
      "| 34.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  48.0|    2.0|  1.0|20160401|     TIA| null|      G|   null|   null|   null| 1968.0|09302016|     M|  null|     AZ|9.247042023E10|00602|      B2|\n",
      "| 35.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  74.0|    2.0|  1.0|20160401|     TIA| null|      T|   null|   null|   null| 1942.0|09302016|     F|  null|     TK|  6.69712185E8|    1|      B2|\n",
      "| 36.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20561.0|  37.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1979.0|09302016|     M|  null|     TK|9.250625823E10|00001|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_test_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat' \n",
    "df_immigration_test = clean_i94_data(immigration_test_file)\n",
    "df_immigration_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean temperature data\n",
    "df_temp=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "\n",
    "# Filter out entries with NaN average temperature\n",
    "df_temp=df_temp.filter(df_temp.AverageTemperature != 'NaN')\n",
    "\n",
    "# Remove duplicate locations\n",
    "df_temp=df_temp.dropDuplicates(['City', 'Country'])\n",
    "\n",
    "@udf()\n",
    "def get_i94port(city):\n",
    "    '''\n",
    "    Input: City name\n",
    "    \n",
    "    Output: Corresponding i94port\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for key in i94port_valid:\n",
    "        if city.lower() in i94port_valid[key][0].lower():\n",
    "            return key\n",
    "\n",
    "# Add iport94 code based on city name\n",
    "df_temp=df_temp.withColumn(\"i94port\", get_i94port(df_temp.City))\n",
    "\n",
    "# Remove entries with no iport94 code\n",
    "df_temp=df_temp.filter(df_temp.i94port != 'null')\n",
    "\n",
    "# Show results\n",
    "# df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "|        dt| AverageTemperature|AverageTemperatureUncertainty|     City|             Country|Latitude|Longitude|i94port|\n",
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "|1852-07-01|             15.488|                        1.395|    Perth|           Australia|  31.35S|  114.97E|    PER|\n",
      "|1828-01-01|             -1.977|                        2.551|  Seattle|       United States|  47.42N|  121.97W|    SEA|\n",
      "|1743-11-01|              2.767|                        1.905| Hamilton|              Canada|  42.59N|   80.73W|    HAM|\n",
      "|1849-01-01|  7.399999999999999|                        2.699|  Ontario|       United States|  34.56N|  116.76W|    ONT|\n",
      "|1821-11-01|              2.322|                        2.375|  Spokane|       United States|  47.42N|  117.24W|    SPO|\n",
      "|1843-01-01| 18.874000000000002|                        2.017|Abu Dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "|1824-01-01|             25.229|                        1.094|    Anaco|           Venezuela|   8.84N|   64.05W|    ANA|\n",
      "|1855-05-01|              9.904|           1.4369999999999998|      Ica|                Peru|  13.66S|   75.14W|    CHI|\n",
      "|1835-01-01|              9.833|                        2.182|  Nogales|       United States|  31.35N|  111.20W|    NOG|\n",
      "|1743-11-01|  8.129999999999999|                        2.245|  Atlanta|       United States|  34.56N|   83.68W|    ATL|\n",
      "|1796-01-01|             15.552|                        2.305|      Mau|               India|  26.52N|   84.18E|    OGG|\n",
      "|1743-11-01|              3.264|                        1.665|   Newark|       United States|  40.99N|   74.56W|    NEW|\n",
      "|1857-01-01| 18.581000000000003|           1.8119999999999998|  Springs|        South Africa|  26.52S|   28.66E|    PSP|\n",
      "|1856-01-01| 26.055999999999997|           1.3769999999999998|      Ise|             Nigeria|   7.23N|    5.68E|    BOI|\n",
      "|1743-11-01|             18.722|                        2.302|  Orlando|       United States|  28.13N|   80.91W|    ORL|\n",
      "|1823-01-01|             11.602|           2.8160000000000003|   Laredo|       United States|  28.13N|   99.09W|    LCB|\n",
      "|1841-01-01| 13.107999999999999|                        2.519|     Tali|              Taiwan|  24.92N|  120.59E|    MET|\n",
      "|1828-01-01|-2.7630000000000003|                        2.617| Victoria|              Canada|  49.03N|  122.45W|    VIC|\n",
      "|1743-11-01| 1.1880000000000002|                        1.531|   Boston|       United States|  42.59N|   72.00W|    BOS|\n",
      "|1849-01-01|  8.091999999999999|           2.1919999999999997|Fairfield|       United States|  37.78N|  122.03W|    FTF|\n",
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the Data Model\n",
    "\n",
    "### 3.1 Conceptual Data Model\n",
    "\n",
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "\n",
    "- `i94yr` = 4 digit year\n",
    "- `i94mon` = numeric month\n",
    "- `i94cit` = 3 digit code of origin city\n",
    "- `i94port` = 3 character code of destination city\n",
    "- `arrdate` = arrival date\n",
    "- `i94mode` = 1 digit travel code\n",
    "- `depdate` = departure date\n",
    "- `i94visa` = reason for immigration\n",
    "\n",
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    "\n",
    "- `i94port` = 3 character code of destination city (mapped from immigration data during cleanup step)\n",
    "- `AverageTemperature` = average temperature\n",
    "- `City` = city name\n",
    "- `Country` = country name\n",
    "- `Latitude` = latitude\n",
    "- `Longitude` = longitude\n",
    "\n",
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on i94port:\n",
    "\n",
    "- `i94yr` = 4 digit year\n",
    "- `i94mon` = numeric month\n",
    "- `i94cit` = 3 digit code of origin city\n",
    "- `i94port` = 3 character code of destination city\n",
    "- `arrdate` = arrival date\n",
    "- `i94mode` = 1 digit travel code\n",
    "- `depdate` = departure date\n",
    "- `i94visa` = reason for immigration\n",
    "- `AverageTemperature` = average temperature of destination city\n",
    "\n",
    "The tables will be saved to Parquet files partitioned by city (i94port).\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The pipeline steps are described below:\n",
    "\n",
    "1. Clean I94 data as described in step 2 to create Spark dataframe df_immigration for each month\n",
    "2. Clean temperature data as described in step 2 to create Spark dataframe df_temp (already performed)\n",
    "3. Create immigration dimension table by selecting relevant columns from df_immigration and write to parquet file partitioned by i94port\n",
    "4. Create temperature dimension table by selecting relevant columns from df_temp and write to parquet file partitioned by i94port\n",
    "5. Create fact table by joining immigration and temperature dimension tables on i94port and write to parquet file partitioned by i94port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Run Pipelines to Model the Data\n",
    "\n",
    "### 4.1 Create the data model\n",
    "*Build the data pipelines to create the data model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to I94 immigration data \n",
    "#immigration_data = '/data/18-83510-I94-Data-2016/*.sas7bdat'\n",
    "immigration_data = '/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "\n",
    "# Clean I94 immigration data and store as Spark dataframe\n",
    "df_immigration = clean_i94_data(immigration_data)\n",
    "\n",
    "# Extract columns for immigration dimension table\n",
    "immigration_table = df_immigration.select([\"i94yr\", \"i94mon\", \"i94cit\", \"i94port\", \"arrdate\", \"i94mode\", \"depdate\", \"i94visa\"])\n",
    "\n",
    "# Write immigration dimension table to parquet files partitioned by i94port\n",
    "immigration_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns for temperature dimension table\n",
    "temp_table = df_temp.select([\"AverageTemperature\", \"City\", \"Country\", \"Latitude\", \"Longitude\", \"i94port\"])\n",
    "\n",
    "# Write temperature dimension table to parquet files partitioned by i94port\n",
    "temp_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views of the immigration and temperature data\n",
    "df_immigration.createOrReplaceTempView(\"immigration_view\")\n",
    "df_temp.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "# Create the fact table by joining the immigration and temperature views\n",
    "fact_table = spark.sql('''\n",
    "SELECT immigration_view.i94yr as year,\n",
    "       immigration_view.i94mon as month,\n",
    "       immigration_view.i94cit as city,\n",
    "       immigration_view.i94port as i94port,\n",
    "       immigration_view.arrdate as arrival_date,\n",
    "       immigration_view.depdate as departure_date,\n",
    "       immigration_view.i94visa as reason,\n",
    "       temp_view.AverageTemperature as temperature,\n",
    "       temp_view.Latitude as latitude,\n",
    "       temp_view.Longitude as longitude\n",
    "FROM immigration_view\n",
    "JOIN temp_view ON (immigration_view.i94port = temp_view.i94port)\n",
    "''')\n",
    "\n",
    "# Write fact table to parquet files partitioned by i94port\n",
    "fact_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/fact.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views of the immigration and temperature data\n",
    "df_immigration.createOrReplaceTempView(\"immigration_view\")\n",
    "df_temp.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "# Create the fact table by joining the immigration and temperature views\n",
    "query = spark.sql('''\n",
    "SELECT immigration_view.i94yr AS year,\n",
    "       immigration_view.i94mon AS month,\n",
    "       immigration_view.i94cit AS city,\n",
    "       immigration_view.i94port AS i94port,\n",
    "       immigration_view.arrdate AS arrival_date,\n",
    "       immigration_view.depdate AS departure_date,\n",
    "       immigration_view.i94visa AS reason,\n",
    "       temp_view.AverageTemperature AS temperature,\n",
    "       temp_view.Latitude AS latitude,\n",
    "       temp_view.Longitude AS longitude\n",
    "FROM immigration_view\n",
    "JOIN temp_view ON immigration_view.i94port = temp_view.i94port\n",
    "WHERE temp_view.AverageTemperature > 5 \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+------------+--------------+------+-----------------+--------+---------+\n",
      "|  year|month| city|i94port|arrival_date|departure_date|reason|      temperature|latitude|longitude|\n",
      "+------+-----+-----+-------+------------+--------------+------+-----------------+--------+---------+\n",
      "|2016.0|  4.0|111.0|    SNA|     20545.0|       20547.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|114.0|    SNA|     20545.0|       20562.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|117.0|    SNA|     20545.0|       20559.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|129.0|    SNA|     20545.0|       20548.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|575.0|    SNA|     20545.0|       20547.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|575.0|    SNA|     20545.0|       20547.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|577.0|    SNA|     20545.0|       20550.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|577.0|    SNA|     20545.0|       20560.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20560.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|          null|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20555.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20567.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|          null|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20565.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20612.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20653.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20547.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20548.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20548.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20548.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "+------+-----+-----+-------+------------+--------------+------+-----------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+------------+--------------+------+-----------------+--------+---------+\n",
      "|  year|month| city|i94port|arrival_date|departure_date|reason|      temperature|latitude|longitude|\n",
      "+------+-----+-----+-------+------------+--------------+------+-----------------+--------+---------+\n",
      "|2016.0|  4.0|111.0|    SNA|     20545.0|       20547.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|114.0|    SNA|     20545.0|       20562.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|117.0|    SNA|     20545.0|       20559.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|129.0|    SNA|     20545.0|       20548.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|575.0|    SNA|     20545.0|       20547.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|575.0|    SNA|     20545.0|       20547.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|577.0|    SNA|     20545.0|       20550.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|577.0|    SNA|     20545.0|       20560.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20560.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|          null|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20555.0|   1.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20567.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|          null|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20565.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20612.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20653.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20547.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20548.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20548.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "|2016.0|  4.0|582.0|    SNA|     20545.0|       20548.0|   2.0|7.168999999999999|  29.74N|   97.85W|\n",
      "+------+-----+-----+-------+------------+--------------+------+-----------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample query that allows users to look at immigration events and temperature based on\n",
    "# conditions above 5 degrees\n",
    "result_df = spark.sql('''\n",
    "SELECT immigration_view.i94yr AS year,\n",
    "       immigration_view.i94mon AS month,\n",
    "       immigration_view.i94cit AS city,\n",
    "       immigration_view.i94port AS i94port,\n",
    "       immigration_view.arrdate AS arrival_date,\n",
    "       immigration_view.depdate AS departure_date,\n",
    "       immigration_view.i94visa AS reason,\n",
    "       temp_view.AverageTemperature AS temperature,\n",
    "       temp_view.Latitude AS latitude,\n",
    "       temp_view.Longitude AS longitude\n",
    "FROM immigration_view\n",
    "JOIN temp_view ON immigration_view.i94port = temp_view.i94port\n",
    "WHERE temp_view.AverageTemperature > 5\n",
    "''')\n",
    "\n",
    "# Display the result DataFrame\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Queries on Database\n",
    "\n",
    "**Question:** Do warmers cities attract more visitors than colder cities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 1 Shows visits to American citites where Average temprature > 5. \n",
    "query = spark.sql('''\n",
    "SELECT immigration_view.i94port AS i94port,\n",
    "       COUNT(*) AS visit_count\n",
    "FROM immigration_view\n",
    "JOIN temp_view ON immigration_view.i94port = temp_view.i94port\n",
    "WHERE temp_view.AverageTemperature > 5\n",
    "GROUP BY immigration_view.i94port\n",
    "ORDER BY visit_count DESC\n",
    "LIMIT 10\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|i94port|visit_count|\n",
      "+-------+-----------+\n",
      "|    LOS|     930489|\n",
      "|    NYC|     485916|\n",
      "|    MIA|     343941|\n",
      "|    SFR|     152586|\n",
      "|    ORL|     149195|\n",
      "|    NEW|     136122|\n",
      "|    CHI|     130564|\n",
      "|    HOU|     101481|\n",
      "|    FTL|      95977|\n",
      "|    ATL|      92579|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 2 Shows visits to American citites where Average temprature < 2. \n",
    "query2 = spark.sql('''\n",
    "SELECT immigration_view.i94port AS i94port,\n",
    "       COUNT(*) AS visit_count\n",
    "FROM immigration_view\n",
    "JOIN temp_view ON immigration_view.i94port = temp_view.i94port\n",
    "WHERE temp_view.AverageTemperature < 2\n",
    "GROUP BY immigration_view.i94port\n",
    "ORDER BY visit_count DESC\n",
    "LIMIT 10\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|i94port|visit_count|\n",
      "+-------+-----------+\n",
      "|    BOS|      57354|\n",
      "|    SEA|      47719|\n",
      "|    TOR|      20886|\n",
      "|    MON|       6006|\n",
      "|    BUF|       1040|\n",
      "|    OTT|        663|\n",
      "|    VIC|        626|\n",
      "|    ANC|         91|\n",
      "|    SYR|         76|\n",
      "|    PRO|         12|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Based on the queries above, it can be seen that citites with warmer climates tend to have higher immigration rates than those with colder temperatures*\n",
    "\n",
    "The top 10 warmer cities visits range from **92,579 - 930,489**. This is significantly higher than countries with colder climates where number of visits range from **12 - 57,354**\n",
    "\n",
    "This is an example of how the schema can be used to answer the question of if temperature affects the immigration rates of a country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Quality Checks\n",
    "*Ensure there are adequate number of entries in each table.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_check(df, description):\n",
    "    '''\n",
    "    Input: Spark dataframe, description of Spark datafram\n",
    "    \n",
    "    Output: Print outcome of data quality check\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for immigration table with 3088544 records\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Perform data quality check\n",
    "print(quality_check(df_immigration, \"immigration table\"))\n",
    "#print(quality_check(df_temp, \"temperature table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df, description):\n",
    "    '''\n",
    "    Input: Spark DataFrame, description of the DataFrame\n",
    "    \n",
    "    Output: Print outcome of duplicate records check\n",
    "    '''\n",
    "    duplicate_count = df.count() - df.dropDuplicates().count()\n",
    "    if duplicate_count == 0:\n",
    "        print(\"Duplicate records check passed for '{}' with no duplicates\".format(description))\n",
    "    else:\n",
    "        print(\"Duplicate records check failed for '{}' with {} duplicate records\".format(description, duplicate_count))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate records check passed for 'immigration table' with no duplicates\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(check_duplicates(df_immigration, \"immigration table\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data dictionary\n",
    "\n",
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "\n",
    "- `i94yr` = 4 digit year\n",
    "- `i94mon` = numeric month\n",
    "- `i94cit` = 3 digit code of origin city\n",
    "- `i94port` = 3 character code of destination city\n",
    "- `arrdate` = arrival date\n",
    "- `i94mode` = 1 digit travel code\n",
    "- `depdate` = departure date\n",
    "- `i94visa` = reason for immigration\n",
    "\n",
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    "\n",
    "- `i94port` = 3 character code of destination city (mapped from immigration data during cleanup step)\n",
    "- `AverageTemperature` = average temperature\n",
    "- `City` = city name\n",
    "- `Country` = country name\n",
    "- `Latitude` = latitude\n",
    "- `Longitude` = longitude\n",
    "\n",
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on i94port:\n",
    "\n",
    "- `i94yr` = 4 digit year\n",
    "- `i94mon` = numeric month\n",
    "- `i94cit` = 3 digit code of origin city\n",
    "- `i94port` = 3 character code of destination city\n",
    "- `arrdate` = arrival date\n",
    "- `i94mode` = 1 digit travel code\n",
    "- `depdate` = departure date\n",
    "- `i94visa` = reason for immigration\n",
    "- `AverageTemperature` = average temperature of destination city\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Project Summary\n",
    "\n",
    "### Rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "**Tools**\n",
    "- **`Spark:`** *Spark was chosen since it can easily handle multiple file formats (including SAS) containing large amounts of data.* \n",
    "- **`Spark SQL:`** *Spark SQL was chosen to process the large input files into dataframes and manipulated via standard SQL join operations to form additional tables.*\n",
    "\n",
    "**Update Frequency**\n",
    "*Monthly.* This is in line with the current raw file format.\n",
    "\n",
    "**Future Design Considerations**\n",
    "\n",
    "1. *The data was increased by 100x.*\n",
    "\n",
    "If Spark with standalone server mode can not process 100x data set, we could consider to put data in `AWS EMR` which is a distributed data cluster for processing large data sets on cloud\n",
    "\n",
    "2. *The data populates a dashboard that must be updated on a daily basis by 7am every day.*\n",
    "\n",
    "Apache Airflow could be used for building up a ETL data pipeline to regularly update the date and populate a report. Apache Airflow also integrate with Python and AWS very well. More applications can be combined together to deliever more powerful task automation.\n",
    "\n",
    "3. *The database needed to be accessed by 100+ people.*\n",
    "\n",
    "If the database needed to be accessed by 100+ people, we could consider publishing the parquet files to HDFS and giving read access to users that need it. If the users want to run SQL queries on the raw data, we could consider publishing to HDFS using a tool such as Impala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
